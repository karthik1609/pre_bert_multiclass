{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kr/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/kr/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "pd.options.mode.chained_assignment = None\n",
    "%config Completer.use_jedi = False\n",
    "import re\n",
    "import fasttext\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "d2vmodel = fasttext.load_model('cc.en.300.bin')\n",
    "\n",
    "import spacy\n",
    "import spacy_stanza\n",
    "from spacy_fastlang import LanguageDetector\n",
    "\n",
    "def _proc_text(text, nlp):\n",
    "    text_lang, text_clnd, text_norm, text_lemm = 'xx', '', '', ''\n",
    "\n",
    "    if text is None:\n",
    "        return [text_lang, text_clnd, text_norm, text_lemm]\n",
    "\n",
    "    text = str(text)\n",
    "    #text = text.lower()\n",
    "    text = re.sub(r'(\\d+)', r' \\1 ', text)\n",
    "    text = text+'.'\n",
    "    text = re.sub('[\\!\\?\\n\\.]+', '. ', text)\n",
    "    text = re.sub('[^a-zA-Z0-9\\'\\.]', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub('(\\s\\.)', '.', text)\n",
    "    text = text.strip()\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    sentences_clnd, sentences_norm, sentences_lemm = [], [], []\n",
    "    for sentence in doc.sents:\n",
    "        cleaned, normalized, lemmatized = [], [], []\n",
    "        for word in sentence:\n",
    "            print(word, word.norm_, word.lemma_)\n",
    "            word_norm = word.norm_.lower()\n",
    "            word_lemm = word.lemma_.lower()\n",
    "            word_clean = re.sub('[^a-zA-Z0-9]', '', word_norm).strip()\n",
    "            if word_norm == '\\'s' and word_lemm == 'be':\n",
    "                cleaned.append('is')\n",
    "            elif word_norm == 'not' and word_lemm == 'n\\'t':\n",
    "                cleaned.append('not')\n",
    "                normalized.append('not')\n",
    "                lemmatized.append('not')\n",
    "            elif len(word_clean) < 2:\n",
    "                continue\n",
    "            elif (word.is_stop is False or word_clean in ['no', 'not'] or word.dep_ == 'neg'):\n",
    "                cleaned.append(word_clean)\n",
    "                normalized.append(word_norm)\n",
    "                lemmatized.append(word_lemm)\n",
    "            else:\n",
    "                cleaned.append(word_clean)\n",
    "\n",
    "        sentence_clnd = ' '.join(cleaned)\n",
    "        sentence_clnd = re.sub('\\s+', ' ', sentence_clnd).strip()\n",
    "        if len(sentence_clnd) > 0:\n",
    "            sentences_clnd.append(sentence_clnd)\n",
    "\n",
    "        sentence_norm = ' '.join(normalized)\n",
    "        sentence_norm = re.sub('\\s+', ' ', sentence_norm).strip()\n",
    "        if len(sentence_norm) > 0:\n",
    "            sentences_norm.append(sentence_norm)\n",
    "\n",
    "        sentence_lemm = ' '.join(lemmatized)\n",
    "        sentence_lemm = re.sub('\\s+', ' ', sentence_lemm).strip()\n",
    "        if len(sentence_lemm) > 0:\n",
    "            sentences_lemm.append(sentence_lemm)\n",
    "\n",
    "    if len(sentences_clnd):\n",
    "        text_clnd = ' . '.join(sentences_clnd)+' .'\n",
    "        text_lang = doc._.language\n",
    "    if len(sentences_norm):\n",
    "        text_norm = ' . '.join(sentences_norm)+' .'\n",
    "    if len(sentences_lemm):\n",
    "        text_lemm = ' . '.join(sentences_lemm)+' .'\n",
    "\n",
    "    return [text_lang, text_clnd, text_norm, text_lemm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['language_detector', 'tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'sentencizer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "nlp_cln = spacy.load('en_core_web_md', disable=['ner', 'parser', 'spacytextblob'])\n",
    "nlp_cln.add_pipe('language_detector', first=True)\n",
    "nlp_cln.add_pipe('sentencizer', last=True)\n",
    "\n",
    "print(nlp_cln.pipe_names)\n",
    "\n",
    "#text = \"apps tickets\"\n",
    "\n",
    "#print(_proc_text(text, nlp_cln))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy #load spacy\n",
    "from spacy import displacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "from nltk import Tree\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "def opinion_mining_method1(sentence):\n",
    "    '''\n",
    "    input: dictionary and sentence\n",
    "    function: appends dictionary with new features if the feature\n",
    "              did not exist previously,then updates sentiment to\n",
    "              each of the new or existing features\n",
    "    output: updated dictionary\n",
    "    '''\n",
    "    sentence = nlp(sentence)\n",
    "    #displacy.render(sentence, style='dep')\n",
    "    \n",
    "    def tok_format(tok):\n",
    "        return \"_\".join([tok.orth_, tok.pos_])\n",
    "\n",
    "\n",
    "    def to_nltk_tree(node):\n",
    "        if node.n_lefts + node.n_rights > 0:\n",
    "            return Tree(tok_format(node), [to_nltk_tree(child) for child in node.children])\n",
    "        else:\n",
    "            return tok_format(node)\n",
    "    \n",
    "    [to_nltk_tree(sent.root).pretty_print() for sent in sentence.sents]\n",
    "\n",
    "    opinion_words = dict()\n",
    "\n",
    "    for assessment in sentence._.assessments:\n",
    "        sentiment_info = dict()\n",
    "        sentiment_info['keyword'] = assessment[0][-1]\n",
    "        sentiment_info['assessment'] = ' '.join(assessment[0])\n",
    "        sentiment_info['score'] = assessment[1]\n",
    "        opinion_words[assessment[0][-1]] = sentiment_info\n",
    "                \n",
    "    def get_target_tokens(token):\n",
    "        target_tokens = []\n",
    "        for child in token.children:\n",
    "            #if child.dep_ in ['acl', 'relcl', 'compound', 'attr', 'xcomp']:\n",
    "            if child.dep_ not in ['advmod', 'amod'] and child.pos_ not in ['ADP', 'AUX', 'CONJ', 'DET', 'NUM', 'PART', 'PRON', 'PUNCT', 'SCONJ', 'SYM', 'X']:\n",
    "                target_tokens.append(child.text)\n",
    "\n",
    "        target_tokens.append(token.text)\n",
    "\n",
    "        #if token.head.dep_ in ['acl', 'relcl', 'compound', 'attr', 'xcomp', 'dobj']:\n",
    "        if token.head.dep_ not in ['advmod', 'amod'] and token.head.pos_ not in ['ADP', 'AUX', 'CONJ', 'DET', 'NUM', 'PART', 'PRON', 'PUNCT', 'SCONJ', 'SYM', 'X']:\n",
    "            target_tokens.append(token.head.text)\n",
    "            for child in token.head.children:\n",
    "                #if child.dep_ in ['acl', 'relcl', 'compound', 'attr']:\n",
    "                if child.dep_ not in ['advmod', 'amod'] and child.pos_ not in ['ADP', 'AUX', 'CONJ', 'DET', 'NUM', 'PART', 'PRON', 'PUNCT', 'SCONJ', 'SYM', 'X'] and child.text not in target_tokens:\n",
    "                    target_tokens.append(child.text)\n",
    "\n",
    "        return target_tokens\n",
    "    \n",
    "    print(opinion_words)    \n",
    "    \n",
    "    all_sentiments = dict()\n",
    "    for token in sentence:\n",
    "        #print(token.text, token.head.text, token.dep_, token.pos_)\n",
    "        # check if the word is an opinion word, then assign sentiment\n",
    "        if token.text in opinion_words:\n",
    "            target_tokens = []\n",
    "            assessment_tokens = []\n",
    "            sentiment = 0 #if token.text in pos else -1\n",
    "            # if target is an adverb modifier (i.e. pretty, highly, etc.)\n",
    "            # but happens to be an opinion word, ignore and pass\n",
    "            if (token.dep_ == \"advmod\"):\n",
    "                continue\n",
    "            elif (token.dep_ == \"amod\"):\n",
    "                target_tokens = get_target_tokens(token.head)\n",
    "                sent_dict = dict()\n",
    "                sent_dict['target'] = ' '.join(target_tokens)\n",
    "                sent_dict['assessment'] = opinion_words[token.text]['assessment'].strip()\n",
    "                sent_dict['sentiment'] = opinion_words[token.text]['score']\n",
    "                sent_dict['debug'] = 0\n",
    "                all_sentiments[token.head.text] = sent_dict\n",
    "            # for opinion words that are adjectives, adverbs, verbs...\n",
    "            else:\n",
    "\n",
    "                for child in token.children:\n",
    "                    # if there's a adj modifier (i.e. very, pretty, etc.) add more weight to sentiment\n",
    "                    # This could be better updated for modifiers that either positively or negatively emphasize\n",
    "                    if ((child.dep_ == \"amod\") or (child.dep_ == \"advmod\")) and (child.text in opinion_words):\n",
    "                        sentiment *= 1.5\n",
    "                    # check for negation words and flip the sign of sentiment\n",
    "                    if child.dep_ == \"neg\":\n",
    "                        sentiment *= -1\n",
    "                for child in token.children:\n",
    "                    # if verb, check if there's a direct object\n",
    "                    if (token.pos_ == \"VERB\") & (child.dep_ == \"dobj\"):                        \n",
    "                        sent_dict = dict()\n",
    "                        sent_dict['target'] = child.text\n",
    "                        sent_dict['assessment'] = opinion_words[token.text]['assessment'].strip()\n",
    "                        sent_dict['sentiment'] = opinion_words[token.text]['score']\n",
    "                        sent_dict['debug'] = 1\n",
    "                        all_sentiments[child.text] = sent_dict\n",
    "                        #sent_dict[child.text] = sentiment\n",
    "                        # check for conjugates (a AND b), then add both to dictionary\n",
    "                        subchildren = []\n",
    "                        conj = 0\n",
    "                        for subchild in child.children:\n",
    "                            if subchild.text == \"and\":\n",
    "                                conj=1\n",
    "                            if (conj == 1) and (subchild.text != \"and\"):\n",
    "                                subchildren.append(subchild.text)\n",
    "                                conj = 0\n",
    "                        for subchild in subchildren:\n",
    "                            sent_dict = dict()\n",
    "                            sent_dict['target'] = subchild\n",
    "                            sent_dict['assessment'] = opinion_words[token.text]['assessment'].strip()\n",
    "                            sent_dict['sentiment'] = opinion_words[token.text]['score']\n",
    "                            sent_dict['debug'] = 2\n",
    "                            all_sentiments[subchild] = sent_dict\n",
    "\n",
    "                # check for negation\n",
    "                for child in token.head.children:\n",
    "                    if ((child.dep_ == \"amod\") or (child.dep_ == \"advmod\")) and (child.text in opinion_words):\n",
    "                        sentiment *= 1.5\n",
    "                    # check for negation words and flip the sign of sentiment\n",
    "                    if (child.dep_ == \"neg\"): \n",
    "                        sentiment *= -1\n",
    "                \n",
    "                # check for nouns\n",
    "                for child in token.head.children:\n",
    "                    noun = \"\"\n",
    "                    if (child.pos_ == \"NOUN\") and (child.text not in all_sentiments):\n",
    "                        noun = child.text\n",
    "                        # Check for compound nouns\n",
    "                        for subchild in child.children:\n",
    "                            if subchild.dep_ == \"compound\":\n",
    "                                noun = subchild.text + \" \" + noun\n",
    "                        sent_dict = dict()\n",
    "                        sent_dict['target'] = noun\n",
    "                        sent_dict['assessment'] = opinion_words[token.text]['assessment'].strip()\n",
    "                        sent_dict['sentiment'] = opinion_words[token.text]['score']\n",
    "                        sent_dict['debug'] = 3\n",
    "                        all_sentiments[noun] = sent_dict\n",
    "                        \n",
    "                if not all_sentiments:\n",
    "                    sent_dict = dict()\n",
    "                    sent_dict['target'] = 'app'\n",
    "                    sent_dict['assessment'] = opinion_words[token.text]['assessment'].strip()\n",
    "                    sent_dict['sentiment'] = opinion_words[token.text]['score']\n",
    "                    sent_dict['debug'] = 4\n",
    "                    all_sentiments[noun] = sent_dict\n",
    "    \n",
    "    return all_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ride_NOUN            \n",
      "    _________|__________       \n",
      "   |         |       app_NOUN \n",
      "   |         |          |      \n",
      "Nice_ADJ but_CCONJ glitchy_ADJ\n",
      "\n",
      "{'nice': {'keyword': 'nice', 'assessment': 'nice', 'score': 0.6}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Nice ride but glitchy app\"\n",
    "#text = \"Super expensive ..they looted me for 500 rupees for just 10 minutes ride..\"\n",
    "#text = \"Yulu had not gave my refund money which paid me though payu but money didn't receive to bank a/c if I didn't receive my yulu refund I will file an FIR\"\n",
    "#text = they are looting miney in the super saver packs. No charged vehicle are available near peak point. Bad service high prices. Complained but only excuses. Why to charge per minute in banglore traffic, better to walk or owe cycle instead of spending it on yulu. High price low service.\"\n",
    "#text = \"Great stupid developer making this app, Developer making this app only for owner, nothing any solution for customer.\"\n",
    "#text = \"Best battery bike ever worth off money \\ud83d\\udcb8 \\ud83d\\udcb8\\ud83d\\udcb8 I had used in banglore... For one day it's very useful... Am in chennai.... But yulu is not there in chennai kindly make tie up with this government it's very useful for us........\"\n",
    "#text = \"Best for daily commuters in Bengaluru\"\n",
    "#text = \"Nice experience. Arrangement is clear and smooth. Cycle quality is good. Only suggestion to indicate battery life once ride starts.\"\n",
    "#text = \"Great ride i reached office on time today exactly at 10am... Fabulous i loved the ride... Charges should be less... Its not distinguished how the fare is calculated...\"\n",
    "#text = \"Good for nearby places.....but your charges is high for an electric vehicle. It is like 1rs per min....pls consider\"\n",
    "#text = \"App is crashing like anything even after updating it..\"\n",
    "#text = \"Waste of money 3rd class app very bad service\"\n",
    "opinion_mining_method1(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import Tree\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "import stanza\n",
    "#import spacy-stanza\n",
    "\n",
    "def get_dep_node_stanza(finaltxt):\n",
    "    nlp = stanza.Pipeline(lang='en', processors={'tokenize': 'spacy'})\n",
    "    doc = nlp(finaltxt)  # Object of Stanza NLP Pipeleine\n",
    "\n",
    "    # Getting the dependency relations betwwen the words\n",
    "    dep_node = []\n",
    "    for dep_edge in doc.sentences[0].dependencies:\n",
    "        dep_node.append([dep_edge[2].text, dep_edge[0].id, dep_edge[1]])\n",
    "\n",
    "    return dep_node\n",
    "\n",
    "def get_dep_node_spacy(finaltxt):\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    #nlp.add_pipe('spacytextblob')\n",
    "\n",
    "    doc = nlp(finaltxt)  # Object of Stanza NLP Pipeleine\n",
    "\n",
    "    # Getting the dependency relations betwwen the words\n",
    "    dep_node = []\n",
    "    for dep_edge in doc.sents[0].dependencies:\n",
    "        dep_node.append([dep_edge[2].text, dep_edge[0].id, dep_edge[1]])\n",
    "\n",
    "    return dep_node\n",
    "\n",
    "def opinion_mining_method2(txt, stop_words, nlp='stanza'):\n",
    "    catNN = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "    \n",
    "    txt = txt.lower() # LowerCasing the given Text\n",
    "    sentList = nltk.sent_tokenize(txt) # Splitting the text into sentences\n",
    "\n",
    "    fcluster = []\n",
    "    totalfeatureList = []\n",
    "    finalcluster = []\n",
    "    dic = {}\n",
    "    print(sentList)\n",
    "\n",
    "    for line in sentList:\n",
    "        newtaggedList = []\n",
    "        txt_list = nltk.word_tokenize(line) # Splitting up into words\n",
    "        taggedList = nltk.pos_tag(txt_list) # Doing Part-of-Speech Tagging to each word\n",
    "\n",
    "        newwordList = []\n",
    "        flag = 0\n",
    "        for i in range(0,len(taggedList)-1):\n",
    "            print(taggedList[i])\n",
    "            if(taggedList[i][1] in catNN and taggedList[i+1][1] in catNN): # If two consecutive words are Nouns then they are joined together\n",
    "                newwordList.append(taggedList[i][0]+'_'+taggedList[i+1][0])\n",
    "                flag=1\n",
    "            else:\n",
    "                if(flag==1):\n",
    "                    flag=0\n",
    "                    continue\n",
    "                newwordList.append(taggedList[i][0])\n",
    "                if(i==len(taggedList)-2):\n",
    "                    print(taggedList[i+1])\n",
    "                    newwordList.append(taggedList[i+1][0])\n",
    "\n",
    "        finaltxt = ' '.join(word for word in newwordList) \n",
    "        new_txt_list = nltk.word_tokenize(finaltxt)\n",
    "        wordsList = [w for w in new_txt_list if not w in stop_words]\n",
    "        taggedList = nltk.pos_tag(wordsList)\n",
    "        \n",
    "        dep_node = []\n",
    "        if nlp == 'stanza':\n",
    "            dep_node = get_dep_node_stanza(finaltxt)\n",
    "        elif nlp == 'stanfordnlp':\n",
    "            dep_node = get_dep_node_stanfordnlp(finaltxt)\n",
    "        elif nlp == 'spacy':\n",
    "            dep_node = get_dep_node_spacy(finaltxt)\n",
    "            \n",
    "        # Coverting it into appropriate format\n",
    "        for i in range(0, len(dep_node)):\n",
    "            if (int(dep_node[i][1]) != 0):\n",
    "                dep_node[i][1] = newwordList[(int(dep_node[i][1]) - 1)]\n",
    "        print(dep_node, '\\n')\n",
    "        \n",
    "        featureList = []\n",
    "        for i in taggedList:\n",
    "            if(i[1]=='JJ' or i[1]=='JJR' or i[1]=='RB' or i[1] in catNN):\n",
    "                featureList.append(list(i)) # For features for each sentence\n",
    "                totalfeatureList.append(list(i)) # Stores the features of all the sentences in the text\n",
    "        print(totalfeatureList)\n",
    "\n",
    "        for i in featureList:\n",
    "            filist = []\n",
    "            for j in dep_node:\n",
    "                if((j[0]==i[0] or j[1]==i[0]) and (j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"obl\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"])):\n",
    "                    if(j[0]==i[0]):\n",
    "                        filist.append(j[1])\n",
    "                    else:\n",
    "                        filist.append(j[0])\n",
    "            fcluster.append([i[0], filist])\n",
    "        print(fcluster)\n",
    "            \n",
    "    for i in totalfeatureList:\n",
    "        dic[i[0]] = i[1]\n",
    "    \n",
    "    for i in fcluster:\n",
    "        if(dic[i[0]] in catNN):\n",
    "            finalcluster.append(i)\n",
    "        \n",
    "    return finalcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-07 13:38:41 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | spacy     |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2022-02-07 13:38:41 INFO: Use device: gpu\n",
      "2022-02-07 13:38:41 INFO: Loading: tokenize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nice ride but glitchy app']\n",
      "('nice', 'JJ')\n",
      "('ride', 'NN')\n",
      "('but', 'CC')\n",
      "('glitchy', 'NN')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-07 13:38:41 INFO: Loading: pos\n",
      "2022-02-07 13:38:45 INFO: Loading: lemma\n",
      "2022-02-07 13:38:45 INFO: Loading: depparse\n",
      "2022-02-07 13:38:45 INFO: Loading: sentiment\n",
      "2022-02-07 13:38:45 INFO: Loading: constituency\n",
      "2022-02-07 13:38:46 INFO: Loading: ner\n",
      "2022-02-07 13:38:46 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nice', 'ride', 'amod'], ['ride', 0, 'root'], ['but', 'glitchy_app', 'cc'], ['glitchy_app', 'ride', 'conj']] \n",
      "\n",
      "[['nice', 'JJ'], ['ride', 'NN'], ['glitchy_app', 'NN']]\n",
      "[['nice', ['ride']], ['ride', ['nice']], ['glitchy_app', []]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['ride', ['nice']], ['glitchy_app', []]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = []#stopwords.words('english')\n",
    "\n",
    "text = \"Nice ride but glitchy app\"\n",
    "#text = \"Super expensive ..they looted me for 500 rupees for just 10 minutes ride..\"\n",
    "#text = \"Yulu had not gave my refund money which paid me though payu but money didn't receive to bank a/c if I didn't receive my yulu refund I will file an FIR\"\n",
    "#text = they are looting miney in the super saver packs. No charged vehicle are available near peak point. Bad service high prices. Complained but only excuses. Why to charge per minute in banglore traffic, better to walk or owe cycle instead of spending it on yulu. High price low service.\"\n",
    "#text = \"Great stupid developer making this app, Developer making this app only for owner, nothing any solution for customer.\"\n",
    "#text = \"Best battery bike ever worth off money \\ud83d\\udcb8 \\ud83d\\udcb8\\ud83d\\udcb8 I had used in banglore... For one day it's very useful... Am in chennai.... But yulu is not there in chennai kindly make tie up with this government it's very useful for us........\"\n",
    "#text = \"Best for daily commuters in Bengaluru\"\n",
    "#text = \"Nice experience. Arrangement is clear and smooth. Cycle quality is good. Only suggestion to indicate battery life once ride starts.\"\n",
    "#text = \"Great ride i reached office on time today exactly at 10am... Fabulous i loved the ride... Charges should be less... Its not distinguished how the fare is calculated...\"\n",
    "#text = \"Good for nearby places.....but your charges is high for an electric vehicle. It is like 1rs per min....pls consider\"\n",
    "#text = \"App is crashing like anything even after updating it..\"\n",
    "#text = \"Waste of money 3rd class app very bad service\"\n",
    "opinion_mining_method2(text, stop_words, 'stanza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import spacy\n",
    "import spacy_stanza\n",
    "from spacy import displacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "import nltk\n",
    "from nltk import Tree\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "\n",
    "def get_dependencies_spacy(sentence):\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    #nlp.add_pipe('spacytextblob')\n",
    "\n",
    "    doc = nlp(sentence)  # Object of Stanza NLP Pipeleine\n",
    "\n",
    "    dep_node = []\n",
    "    tokens = doc.to_json()['tokens']\n",
    "    for token in tokens:\n",
    "        dep_node.append([sentence[token['start']:token['end']], token['id'], token['dep']])\n",
    "\n",
    "    return dep_node\n",
    "\n",
    "def get_tokens_spacy(sentence):\n",
    "    #nlp = spacy_stanza.load_pipeline(\"en\")\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    tokens = nlp(sentence)\n",
    "    \n",
    "    sentence_words, sentence_tags = [], []\n",
    "    for token in tokens:\n",
    "        if token.is_stop is False:\n",
    "            sentence_tags.append((token.text, token.tag_))\n",
    "        sentence_words.append(token.text)\n",
    "        \n",
    "    return sentence_words, sentence_tags\n",
    "\n",
    "def get_tokens_nltk(sentence):\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    tag_list = nltk.pos_tag(nltk.word_tokenize(sentence)) # Doing Part-of-Speech Tagging to each word\n",
    "\n",
    "    sentence_words = []\n",
    "    flag = 0\n",
    "    for i in range(0,len(tag_list)-1):\n",
    "        if(tag_list[i][1] in catNN and tag_list[i+1][1] in catNN): # If two consecutive words are Nouns then they are joined together\n",
    "            sentence_words.append(tag_list[i][0]+'_'+tag_list[i+1][0])\n",
    "            flag=1\n",
    "        else:\n",
    "            if(flag==1):\n",
    "                flag=0\n",
    "                continue\n",
    "            sentence_words.append(tag_list[i][0])\n",
    "            if(i==len(tag_list)-2):\n",
    "                print(tag_list[i+1])\n",
    "                sentence_words.append(tag_list[i+1][0])\n",
    "\n",
    "    sentence_text = ' '.join(word for word in sentence_words) \n",
    "    word_list = nltk.word_tokenize(sentence_text)\n",
    "    tag_list = [w for w in word_list if not w in stop_words]\n",
    "    sentence_tags = nltk.pos_tag(tag_list)\n",
    "    \n",
    "    return sentence_words, sentence_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-07 13:38:50 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2022-02-07 13:38:50 INFO: Use device: gpu\n",
      "2022-02-07 13:38:50 INFO: Loading: tokenize\n",
      "2022-02-07 13:38:50 INFO: Loading: pos\n",
      "2022-02-07 13:38:50 INFO: Loading: lemma\n",
      "2022-02-07 13:38:50 INFO: Loading: depparse\n",
      "2022-02-07 13:38:50 INFO: Loading: sentiment\n",
      "2022-02-07 13:38:50 INFO: Loading: constituency\n",
      "2022-02-07 13:38:50 INFO: Loading: ner\n",
      "2022-02-07 13:38:51 INFO: Done loading processors!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacytextblob.spacytextblob.SpacyTextBlob at 0x7f6891770820>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import stanza\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "sentiment_nlp = spacy.load('en_core_web_lg')\n",
    "sentiment_nlp.add_pipe('spacytextblob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-07 13:38:55 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2022-02-07 13:38:55 INFO: Use device: gpu\n",
      "2022-02-07 13:38:55 INFO: Loading: tokenize\n",
      "2022-02-07 13:38:55 INFO: Loading: pos\n",
      "2022-02-07 13:38:55 INFO: Loading: lemma\n",
      "2022-02-07 13:38:55 INFO: Loading: depparse\n",
      "2022-02-07 13:38:55 INFO: Loading: sentiment\n",
      "2022-02-07 13:38:55 INFO: Loading: constituency\n",
      "2022-02-07 13:38:55 INFO: Loading: ner\n",
      "2022-02-07 13:38:56 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import stanza\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "import networkx as nx\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "sentiment_nlp = spacy.load('en_core_web_lg')\n",
    "sentiment_nlp.add_pipe('spacytextblob')\n",
    "\n",
    "catNN = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "\n",
    "def get_graph(doc):\n",
    "    edges = []\n",
    "    for token in doc:\n",
    "        for child in token.children:\n",
    "            edges.append(('{0}'.format(token.lower_), '{0}'.format(child.lower_)))\n",
    "        #edges.append((token[0], token[1]))\n",
    "    #print(edges)\n",
    "    graph = nx.Graph(edges)\n",
    "    return graph\n",
    "    \n",
    "def get_path(graph, aspect, qualifier):\n",
    "    return nx.shortest_path_length(graph, source=aspect, target=qualifier), nx.shortest_path(graph, source=aspect, target=qualifier)\n",
    "\n",
    "\n",
    "def process_stanza(sentence, nlp, stop_words, combine_nouns=False):\n",
    "    doc = nlp(sentence)\n",
    "    if combine_nouns:\n",
    "        org_words = doc.sentences[0].words\n",
    "        new_words = []\n",
    "        j = 0\n",
    "        for i in range(0, len(org_words)):\n",
    "            if org_words[i].xpos in catNN:\n",
    "                if j > 0:\n",
    "                    j = j-1\n",
    "                    continue\n",
    "                word_NN = ''\n",
    "                while(i+j < len(org_words) and org_words[i+j].xpos in catNN):\n",
    "                    word_NN += org_words[i+j].text + '_'\n",
    "                    j += 1\n",
    "                new_words.append(word_NN[0:-1])\n",
    "            else:\n",
    "                new_words.append(org_words[i].text)\n",
    "                j = 0\n",
    "        sentence = ' '.join(new_words) \n",
    "        doc = nlp(sentence)\n",
    "    \n",
    "    words, targets, all_targets = [], {}, []\n",
    "    for word in doc.sentences[0].words:\n",
    "        #print(word.text, word.xpos)\n",
    "        if word.text not in stop_words and word.xpos in catNN:#+['JJ', 'JR', 'RB']:\n",
    "            targets[word.text] = word.xpos\n",
    "            all_targets.append([word.text, word.xpos])\n",
    "        words.append(word.text)\n",
    "    #print(targets, all_targets)\n",
    "        \n",
    "    dependencies = []\n",
    "    for dep_edge in doc.sentences[0].dependencies:\n",
    "        if dep_edge[1] != 'root':\n",
    "            dependencies.append([dep_edge[2].text, dep_edge[0].id, dep_edge[1]])\n",
    "\n",
    "    # Coverting it into appropriate format\n",
    "    for i in range(0, len(dependencies)):\n",
    "        if (int(dependencies[i][1]) != 0):\n",
    "            dependencies[i][1] = words[(int(dependencies[i][1]) - 1)]\n",
    "\n",
    "    return words, targets, dependencies\n",
    "\n",
    "def extract_aspects(sentence, nlp, stop_words, combine_nouns=False):\n",
    "    words, targets, dependencies = process_stanza(sentence, nlp, stop_words, combine_nouns)\n",
    "        \n",
    "    sentence = ' '.join(words)\n",
    "    sentiment = sentiment_nlp(sentence)\n",
    "    sentiment_terms = {}\n",
    "    for item in sentiment._.assessments:\n",
    "        sentiment_terms[item[0][-1]] = [item[1], ' '.join(item[0])]\n",
    "    \n",
    "    sentence_graph = get_graph(sentiment)#dependencies)\n",
    "\n",
    "    dependencies = [dep for dep in dependencies if dep[0] not in stop_words and dep[1] not in stop_words]    \n",
    "\n",
    "    targets_assessments = {}\n",
    "    for target in targets:\n",
    "        if targets[target] not in catNN:#+['JJ','JR','RB']:\n",
    "            continue\n",
    "        target_text = target\n",
    "        assessments = []\n",
    "        for dependency in dependencies:\n",
    "            #if dependency[2] in ['det', 'case', 'root', 'acl:relcl']:\n",
    "            #if not dependency[2] in [\"nsubj\", \"acl\", \"recl\", \"acl:relcl\", \"obj\", \"obl\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"]:\n",
    "            #if not dependency[2] in [\"acl\", \"recl\", \"obj\", \"obl\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"]:\n",
    "            #if not dependency[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"obl\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"]:\n",
    "                #continue\n",
    "            #if dependency[0]==target and dependency[1] in targets:\n",
    "                #target_text = target_text + ' ' + dependency[1]\n",
    "            #elif dependency[1]==target and dependency[0] in targets:\n",
    "                #target_text = target_text + ' ' + dependency[0]\n",
    "            if dependency[0]==target and dependency[1] not in targets:# and dependency[1] not in stop_words:\n",
    "                glength, gpath = get_path(sentence_graph, dependency[0], dependency[1])\n",
    "                #print(glength, gpath)\n",
    "                assessments.append(dependency[1])\n",
    "            elif dependency[1]==target and dependency[0] not in targets:# and dependency[0] not in stop_words:\n",
    "                glength, gpath = get_path(sentence_graph, dependency[1], dependency[0])\n",
    "                #print(glength, gpath)\n",
    "                assessments.append(dependency[0])\n",
    "        if len(assessments) == 0:\n",
    "            continue\n",
    "        if targets[target] in catNN:#+['JJ','JR','RB']:\n",
    "            target_text = \" \".join([token.lemma_ for token in sentiment_nlp(target_text)])\n",
    "            targets_assessments[target] = dict()\n",
    "            targets_assessments[target]['target_text'] = target_text\n",
    "            targets_assessments[target]['assessment_text'] = ' '.join(assessments)\n",
    "            sent_terms = [term for term in assessments if term in sentiment_terms]\n",
    "            if len(sent_terms) == 0:\n",
    "                targets_assessments[target]['sentiment'] = 'unknown'\n",
    "            elif sentiment_terms[sent_terms[0]][0] >= 0.33:\n",
    "                targets_assessments[target]['sentiment'] = 'positive'\n",
    "                #targets_assessments[target]['assessment_text'] = re.sub(sent_terms[0], sentiment_terms[sent_terms[0]][1], targets_assessments[target]['assessment_text'])\n",
    "            elif sentiment_terms[sent_terms[0]][0] <= -0.33:\n",
    "                targets_assessments[target]['sentiment'] = 'negative'\n",
    "                #targets_assessments[target]['assessment_text'] = re.sub(sent_terms[0], sentiment_terms[sent_terms[0]][1], targets_assessments[target]['assessment_text'])\n",
    "            else:\n",
    "                targets_assessments[target]['sentiment'] = 'mixed'\n",
    "                #targets_assessments[target]['assessment_text'] = re.sub(sent_terms[0], sentiment_terms[sent_terms[0]][1], targets_assessments[target]['assessment_text'])\n",
    "\n",
    "    return targets_assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_text</th>\n",
       "      <th>assessment_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>class_app</td>\n",
       "      <td>3rd</td>\n",
       "      <td>mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>service</td>\n",
       "      <td>bad</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target_text assessment_text sentiment\n",
       "0   class_app             3rd     mixed\n",
       "1     service             bad  negative"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text = \"nice yulu ride but glitchy app\"\n",
    "#text = \"Super expensive ..they looted me for 500 rupees for just 10 minutes ride..\"\n",
    "#text = \"yulu had not gave my refund money which paid me though payu but money didn't receive to bank a/c if I didn't receive my yulu refund I will file an fir\"\n",
    "#text = \"they are looting miney in the super saver packs. No charged vehicle are available near peak point. Bad service high prices. Complained but only excuses. Why to charge per minute in banglore traffic, better to walk or owe cycle instead of spending it on yulu. High price low service.\"\n",
    "#text = \"great stupid developer making this app, geveloper making this app only for owner, nothing any solution for customer.\"\n",
    "#text = \"best battery bike ever worth off money \\ud83d\\udcb8 \\ud83d\\udcb8\\ud83d\\udcb8 I had used in banglore... For one day it's very useful... Am in chennai.... But yulu is not there in chennai kindly make tie up with this government it's very useful for us........\"\n",
    "#text = \"best for daily commuters in bengaluru\"\n",
    "#text = \"nice experience. arrangement is clear and smooth. Cycle quality is good. Only suggestion to indicate battery life once ride starts.\"\n",
    "#text = \"great ride i reached office on time today exactly at 10am... Fabulous i loved the ride... Charges should be less... Its not distinguished how the fare is calculated...\"\n",
    "#text = \"good for nearby places.....but your charges is high for an electric vehicle. It is like 1rs per min....pls consider\"\n",
    "#text = \"app is crashing like anything even after updating it..\"\n",
    "text = \"waste of money 3rd class app very bad service\"\n",
    "\n",
    "result = extract_aspects(text, nlp, stop_words, combine_nouns=False)\n",
    "#print(result)\n",
    "\n",
    "result = extract_aspects(text, nlp, stop_words, combine_nouns=True)\n",
    "#print(result)\n",
    "\n",
    "import pandas as pd\n",
    "values = pd.DataFrame(result.values())\n",
    "\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-07 13:38:58 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2022-02-07 13:38:58 INFO: Use device: gpu\n",
      "2022-02-07 13:38:58 INFO: Loading: tokenize\n",
      "2022-02-07 13:38:58 INFO: Loading: pos\n",
      "2022-02-07 13:38:58 INFO: Loading: lemma\n",
      "2022-02-07 13:38:58 INFO: Loading: depparse\n",
      "2022-02-07 13:38:59 INFO: Loading: sentiment\n",
      "2022-02-07 13:38:59 INFO: Loading: constituency\n",
      "2022-02-07 13:38:59 INFO: Loading: ner\n",
      "2022-02-07 13:38:59 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "import spacy_stanza\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "nlp = spacy_stanza.load_pipeline(\"en\")\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "catNN = ['NN', 'NNS', 'NNP', 'NNPS']#+['JJ', 'JR', 'RB']\n",
    "\n",
    "def process_spacy(doc):\n",
    "    words, targets, all_targets = [], {}, []\n",
    "    for word in doc:\n",
    "        if word.tag_ in catNN:\n",
    "            targets[word.text] = [word.tag_, word.lemma_]\n",
    "            all_targets.append([word.text, word.tag_, word.lemma_])\n",
    "        words.append(word.text)\n",
    "        \n",
    "    edges = []\n",
    "    for token in doc:\n",
    "        token_info = [token.text, token.lemma_, (token.is_stop and token.text not in ['no', 'not']), token.pos_, token.tag_]\n",
    "        token_info = [token.text, token.lemma_, False, token.pos_, token.tag_]\n",
    "        for child in token.children:\n",
    "            child_info = [child.text, child.lemma_, (child.is_stop and child.text not in ['no', 'not']), child.pos_, child.tag_]\n",
    "            child_info = [child.text, child.lemma_, False, child.pos_, child.tag_]\n",
    "            edges.append([child.dep_, token_info, child_info])\n",
    "\n",
    "    return words, targets, edges\n",
    "\n",
    "def extract_aspects(sentence, nlp):\n",
    "    doc = nlp(sentence)\n",
    "    words, targets, edges = process_spacy(doc)\n",
    "\n",
    "    sentiment_terms = {}\n",
    "    for item in doc._.assessments:\n",
    "        sentiment_terms[item[0][-1]] = [item[1], ' '.join(item[0])]\n",
    "    #print(sentiment_terms)\n",
    "    targets_assessments = {}\n",
    "    for target in targets:\n",
    "        if targets[target][0] not in catNN:\n",
    "            continue\n",
    "        target_text = targets[target][1]\n",
    "        assessments = []\n",
    "        for edge in edges:\n",
    "            if edge[0] in ['det', 'case', 'root', 'acl:relcl'] and 'no' not in edge[1]+edge[2]:\n",
    "            #if not dependency[2] in [\"nsubj\", \"acl\", \"recl\", \"acl:relcl\", \"obj\", \"obl\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"]:\n",
    "            #if not dependency[2] in [\"acl\", \"recl\", \"obj\", \"obl\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"]:\n",
    "            #if not dependency[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"obl\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"]:\n",
    "                continue\n",
    "            if edge[1][0] == target and edge[2][2] is False:# and dependency[1] not in targets:#\n",
    "                if edge[2][0] in targets:# and dependency[2] in ['conj', 'compound', 'nmod']:\n",
    "                    target_text = target_text + ' ' + edge[2][1]\n",
    "                else:\n",
    "                    assessments.append(edge[2][0])\n",
    "            elif edge[2][0] == target and edge[1][2] is False:# and dependency[0] not in targets:#\n",
    "                if edge[1][0] in targets:# and dependency[2] in ['conj', 'compound', 'nmod']:\n",
    "                    target_text = edge[1][1] + ' ' + target_text\n",
    "                else:\n",
    "                    assessments.append(edge[1][0])\n",
    "            \n",
    "        if len(assessments) == 0:\n",
    "            continue\n",
    "\n",
    "        targets_assessments[target] = dict()\n",
    "        targets_assessments[target]['target_text'] = target_text\n",
    "        targets_assessments[target]['assessment_text'] = assessments\n",
    "        sent_terms = [term for term in assessments if term in sentiment_terms]\n",
    "        if len(sent_terms) == 0:\n",
    "            targets_assessments[target]['sentiment'] = 'unknown'\n",
    "            targets_assessments[target]['sentiment_text'] = []\n",
    "        elif sentiment_terms[sent_terms[0]][0] >= 0.33:\n",
    "            targets_assessments[target]['sentiment'] = 'positive'\n",
    "            targets_assessments[target]['sentiment_text'] = [sentiment_terms[term][1] for term in sent_terms]\n",
    "        elif sentiment_terms[sent_terms[0]][0] <= -0.33:\n",
    "            targets_assessments[target]['sentiment'] = 'negative'\n",
    "            targets_assessments[target]['sentiment_text'] = [sentiment_terms[term][1] for term in sent_terms]\n",
    "        else:\n",
    "            targets_assessments[target]['sentiment'] = 'mixed'\n",
    "            targets_assessments[target]['sentiment_text'] = [sentiment_terms[term][1] for term in sent_terms]\n",
    "\n",
    "    return targets_assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "text = []\n",
    "text.append(\"nice yulu ride but glitchy app\")\n",
    "text.append(\"Super expensive ..they looted me for 500 rupees for just 10 minutes ride..\")\n",
    "text.append(\"yulu had not gave my refund money which paid me though payu but money didn't receive to bank a/c if I didn't receive my yulu refund I will file an fir\")\n",
    "text.append(\"yulu had not gave my refund money which paid me though payu but money didn't receive to bank account if I didn't receive my yulu refund I will file an fir\")\n",
    "text.append(\"they are looting miney in the super saver packs. No charged vehicle are available near peak point. Bad service high prices. Complained but only excuses. Why to charge per minute in banglore traffic, better to walk or owe cycle instead of spending it on yulu. High price low service.\")\n",
    "text.append(\"great stupid developer making this app, geveloper making this app only for owner, nothing any solution for customer.\")\n",
    "#text.append(\"best battery bike ever worth off money \\ud83d\\udcb8 \\ud83d\\udcb8\\ud83d\\udcb8 I had used in banglore... For one day it's very useful... Am in chennai.... But yulu is not there in chennai kindly make tie up with this government it's very useful for us........\")\n",
    "text.append(\"best for daily commuters in bengaluru\")\n",
    "text.append(\"nice experience. arrangement is clear and smooth. Cycle quality is good. Only suggestion to indicate battery life once ride starts.\")\n",
    "text.append(\"great ride i reached office on time today exactly at 10am... Fabulous i loved the ride... Charges should be less... Its not distinguished how the fare is calculated...\")\n",
    "text.append(\"good for nearby places.....but your charges is high for an electric vehicle. It is like 1rs per min....pls consider\")\n",
    "text.append(\"app is crashing like anything even after updating it..\")\n",
    "text.append(\"waste of money 3rd class app very bad service\")\n",
    "#text.append('This is a good comment. All other comments are bad.')\n",
    "#text.append('The new car is good but the mileage is bad.')\n",
    "#text.append('Santa is good but banta is bad')\n",
    "#text.append('Santa and banta are good and bad, respectively')\n",
    "#text.append('I drove the bike.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_text</th>\n",
       "      <th>assessment_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>class_app</td>\n",
       "      <td>3rd</td>\n",
       "      <td>mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>service</td>\n",
       "      <td>bad</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target_text assessment_text sentiment\n",
       "0   class_app             3rd     mixed\n",
       "1     service             bad  negative"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "%time\n",
    "df = pd.read_json('yulu.gplay.json')\n",
    "text = list(df.content)\n",
    "mod_list = []\n",
    "\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "def joiner(x):\n",
    "    x = [x_ for x_ in x if x_ != '.']\n",
    "    return ' '.join(x)\n",
    "\n",
    "count = 0\n",
    "for text_ in text:\n",
    "    if text_ == 'Bhai maza aa gya paise wasul':\n",
    "        text_ = 'value for money'\n",
    "    text_ = deEmojify(text_)\n",
    "    result = extract_aspects(text_, nlp)\n",
    "    values = pd.DataFrame(result.values())\n",
    "    \n",
    "    if len(values):\n",
    "        values['assessment_text'] = values['assessment_text'].apply(lambda x: joiner(x))\n",
    "        values['mod_text'] = values['assessment_text'] + ' ' + values['target_text']\n",
    "        mod_list.append(list(values['mod_text']))\n",
    "    else:\n",
    "        mod_list.append(text_.split('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['mod_list'] = mod_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_json('yulu_w_modlist.gplay.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[['content', 'mod_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('yulu_w_modlist.gplay.json')[['content', 'mod_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "d2vmodel = fasttext.load_model('cc.en.300.bin')\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def vectorize_comments_seq(item):\n",
    "    \n",
    "    word_list = item.split(' ')\n",
    "    seq_vec = []\n",
    "    init = True\n",
    "\n",
    "    for word in word_list:\n",
    "        if init:\n",
    "            vec = d2vmodel.get_word_vector(word)\n",
    "            init = False\n",
    "        else:\n",
    "            vec = np.vstack((vec, d2vmodel.get_word_vector(word)))\n",
    "    if vec.shape == (vec.shape[0],):\n",
    "        vec = vec.reshape(1, vec.shape[0])\n",
    "    return pad_sequences(vec.T, maxlen=100, dtype='float').T  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#lol_phrase_embeddings = [[vectorize_comments_seq(m) for m in mod] for mod in df.mod_list]\n",
    "k = [[]]\n",
    "init = True\n",
    "for mod in df.mod_list:        \n",
    "    for m in mod:\n",
    "        if init:\n",
    "            if m != '': \n",
    "                k = vectorize_comments_seq(m)\n",
    "            else:\n",
    "                k = np.zeros((100, 300))\n",
    "            k = k.reshape(1, 100, 300)\n",
    "        else:\n",
    "            if m != '': \n",
    "                k_ = vectorize_comments_seq(m)\n",
    "            else:\n",
    "                k_ = np.zeros((100, 300))\n",
    "            k_ = k_.reshape(1, 100, 300)\n",
    "            k = np.vstack((k, k_))\n",
    "        init = False\n",
    "        #print(m)\n",
    "#print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 100, 300)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>mod_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>App is crashing like anything even after updat...</td>\n",
       "      <td>[crashing App, crashing anything]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nice ride</td>\n",
       "      <td>[Nice ride]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Super expensive ..they looted me for 500 rupee...</td>\n",
       "      <td>[looted 500 rupee, 10 ride minute, looted ride...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Expensive then petrol vehicle...</td>\n",
       "      <td>[Expensive then ... vehicle petrol]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>For one full charge how much km v can ride</td>\n",
       "      <td>[one full charge, much ride kilometer, ride v]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Unnecessary charges I can't able to pause the ...</td>\n",
       "      <td>[Unnecessary charge, pause ride]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Amazing and comfortable experience</td>\n",
       "      <td>[Amazing experience]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Very important this , next Journey to Electric...</td>\n",
       "      <td>[important , next journey vehicle, Electric jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Great </td>\n",
       "      <td>[Great ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Nice feeling good</td>\n",
       "      <td>[Nice good feeling]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  App is crashing like anything even after updat...   \n",
       "1                                          Nice ride   \n",
       "2  Super expensive ..they looted me for 500 rupee...   \n",
       "3                   Expensive then petrol vehicle...   \n",
       "4         For one full charge how much km v can ride   \n",
       "5  Unnecessary charges I can't able to pause the ...   \n",
       "6                 Amazing and comfortable experience   \n",
       "7  Very important this , next Journey to Electric...   \n",
       "8                                            Great    \n",
       "9                                  Nice feeling good   \n",
       "\n",
       "                                            mod_list  \n",
       "0                  [crashing App, crashing anything]  \n",
       "1                                        [Nice ride]  \n",
       "2  [looted 500 rupee, 10 ride minute, looted ride...  \n",
       "3                [Expensive then ... vehicle petrol]  \n",
       "4     [one full charge, much ride kilometer, ride v]  \n",
       "5                   [Unnecessary charge, pause ride]  \n",
       "6                               [Amazing experience]  \n",
       "7  [important , next journey vehicle, Electric jo...  \n",
       "8                                           [Great ]  \n",
       "9                                [Nice good feeling]  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24400, 300, 1), (1700, 300, 1), (200, 300, 1))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(24400, 300, 1), (1700, 300, 1), (200, 300, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['crashing App', 'crashing anything'] 2\n",
      "['Nice ride'] 1\n",
      "['looted 500 rupee', '10 ride minute', 'looted ride minute'] 3\n",
      "['Expensive then ... vehicle petrol'] 1\n",
      "['one full charge', 'much ride kilometer', 'ride v'] 3\n",
      "['Unnecessary charge', 'pause ride'] 2\n",
      "['Amazing experience'] 1\n",
      "['important , next journey vehicle', 'Electric journey vehicle'] 2\n",
      "['Great '] 1\n",
      "['Nice good feeling'] 1\n"
     ]
    }
   ],
   "source": [
    "for mod in df.mod_list:\n",
    "    print(mod, len(mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
